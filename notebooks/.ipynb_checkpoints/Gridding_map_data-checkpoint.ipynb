{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X lines of Python\n",
    "# Gridding map data\n",
    "\n",
    "This notebook goes with the [Agile Scientific blog post from 8 March 2019](https://agilescientific.com/blog/2019/3/8/x-lines-of-python-gridding-map-data).\n",
    "\n",
    "I'm using a small dataset originally from [**Geoff Bohling**](http://people.ku.edu/~gbohling/) at the Kansas Geological Survey. I can no longer find the data online.\n",
    "\n",
    "We will look at four ways to do this:\n",
    "\n",
    "- Using SciPy with `scipy.interpolate.Rbf`\n",
    "- Using SciPy with `scipy.griddata()`\n",
    "- Using the Scikit-Learn machine learning library with `sklearn.gaussian_process`\n",
    "- Using the [Verde](https://github.com/fatiando/verde) spatial gridding library with `verde.Spline` (thank you to Leo Uieda for contributing this!)\n",
    "\n",
    "\n",
    "\n",
    "## Load and inspect the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll load the data from the web; if you are offline but have the repo, the data file is also in `../data/ZoneA.dat`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential line number 1.\n",
    "df = pd.read_csv('https://www.dropbox.com/s/6dyfc4fl5slhgry/ZoneA.dat?raw=1',\n",
    "                 sep=' ',\n",
    "                 header=9,\n",
    "                 usecols=[0, 1, 2, 3],\n",
    "                 names=['x', 'y', 'thick', 'por'],\n",
    "                 dtype=\"float64\"\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df.por)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks a bit unpleasant, but we're just getting out min and max values for the x and y columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Line 2.\n",
    "extent = x_extent = x_min, x_max, y_min, y_max = [df.x.min()-1000, df.x.max()+1000,\n",
    "                                                  df.y.min()-1000, df.y.max()+1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later on, we'll see a nicer way to do this using the Verde library.\n",
    "\n",
    "Now we can plot the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "\n",
    "ax.scatter(df.x, df.y, c=df.por)\n",
    "ax.set_aspect(1)\n",
    "ax.set_xlim(*extent[:2])\n",
    "ax.set_ylim(*extent[2:])\n",
    "ax.set_xlabel('Easting [m]')\n",
    "ax.set_ylabel('Northing [m]')\n",
    "ax.set_title('Porosity %')\n",
    "ax.grid(c='k', alpha=0.2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a regular grid\n",
    "\n",
    "We must make a grid, which represents the points we'd like to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Line 3.\n",
    "grid_x, grid_y = np.mgrid[x_min:x_max:500, y_min:y_max:500]\n",
    "\n",
    "# Use *shape* argument to specify the *number* (not size) of bins:\n",
    "# grid_x, grid_y = np.mgrid[x_min:x_max:100j, y_min:y_max:100j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(grid_x, grid_y, s=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was easy!\n",
    "\n",
    "## Interpolation with radial basis function\n",
    "\n",
    "Now we make an interpolator and use it to predict into the grid 'cells'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import Rbf\n",
    "\n",
    "# Make an n-dimensional interpolator. This is essential line number 4.\n",
    "rbfi = Rbf(df.x, df.y, df.por)\n",
    "\n",
    "# Predict on the regular grid. Line 5.\n",
    "di = rbfi(grid_x, grid_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the result. First, we'll need the min and max of the combined sparse and gridded data, so we can plot them with the same colourmap ranges:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mi = np.min(np.hstack([di.ravel(), df.por.values]))\n",
    "ma = np.max(np.hstack([di.ravel(), df.por.values]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the transpose and the `origin='lower'`, to keep everything matched up with the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "c1 = plt.imshow(di.T, origin=\"lower\", extent=extent, vmin=mi, vmax=ma)\n",
    "c2 = plt.scatter(df.x, df.y, s=60, c=df.por, edgecolor='#ffffff66', vmin=mi, vmax=ma)\n",
    "\n",
    "plt.colorbar(c1, shrink=0.67)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The circles (the data) are the same colour as the grid (the model), so we can see that the error on this prediction is almost zero. In fact, the default parameters force the model to pass through all the data points (interpolation, as opposed to estimation or approximation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Rbf()` interpolator has [a few options](https://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.Rbf.html). The most important one is probably `smooth`, which is the thing to increase if you end up with a singular matrix (because it can't converge on a solution). Anything above 0 relaxes the constraint that the surface must pass through every point. If you get an error, you probably need to change the smoothing.\n",
    "\n",
    "You can also change the `function` (default is `multiquadric`, which also has an `epsilon` parameter to vary the range of influence of each point)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbfi = Rbf(df.x, df.y, df.por, smooth=0.2)\n",
    "di = rbfi(grid_x, grid_y)\n",
    "\n",
    "plt.imshow(di.T, origin=\"lower\", extent=extent)\n",
    "plt.scatter(df.x, df.y, s=2, c='w')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also make a histogram and kernel density estimation of the errors, by making predictions at the original input locations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "por_hat = rbfi(df.x, df.y)\n",
    "\n",
    "sns.distplot(por_hat - df.por)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the smoothing set to 0.2, we end up with a smoother surface, but pay for it with larger errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpolation with `scipy.griddata()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Rbf()` interpolator is the one to know about, because it has lots of useful parameters. It's probably the only one you need to know. But there is also `scipy.griddata()`. For example see [this SciPy recipe](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.interpolate.griddata.html).\n",
    "\n",
    "The interface is slightly different &mdash; we have to pass a single array of the coordinates (the (x, y) locations of the points we know). We also pass the values to interpolate, and the grids.\n",
    "\n",
    "The function will not accept Pandas `Series` objects, so we'll use the `Series.values` attribute to get at the NumPy array representation.\n",
    "\n",
    "First, let's make the 2D array of coordinates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = df[['x', 'y']].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The grdding step is easy. We'll try three different algorithms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import griddata\n",
    "\n",
    "grid_z0 = griddata(points, df.por.values, (grid_x, grid_y), method='nearest')\n",
    "grid_z1 = griddata(points, df.por.values, (grid_x, grid_y), method='linear')\n",
    "grid_z2 = griddata(points, df.por.values, (grid_x, grid_y), method='cubic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=3, figsize=(15, 5))\n",
    "\n",
    "ax = axs[0]\n",
    "ax.imshow(grid_z0, origin='lower', extent=extent)\n",
    "ax.scatter(df.x, df.y, s=2, c='w')\n",
    "ax.set_title('Nearest')\n",
    "\n",
    "ax = axs[1]\n",
    "ax.imshow(grid_z1, origin='lower', extent=extent)\n",
    "ax.scatter(df.x, df.y, s=2, c='w')\n",
    "ax.set_title('Linear')\n",
    "\n",
    "ax = axs[2]\n",
    "ax.imshow(grid_z2, origin='lower', extent=extent)\n",
    "ax.scatter(df.x, df.y, s=2, c='w')\n",
    "ax.set_title('Cubic')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't particularly like any of these results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using `sklearn.gaussian_process`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modeling with a Gaussian process is equivalent to kriging. Conveniently, the popular machine learning library `scikit-learn` has a Gaussian process modeling tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process.kernels import RBF\n",
    "\n",
    "kernel = RBF(length_scale=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main hyperparameters are the kernel, which we just defined, and `alpha`, which controls the smoothness. Larger values imply mmore noise in the input data, and result in smoother grids; default is very small: 1 &times; 10<sup>-9</sup>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "\n",
    "gp = GaussianProcessRegressor(normalize_y=True,\n",
    "                              alpha=0.1,  # Larger values imply more noise in the input data.\n",
    "                              kernel=kernel,)\n",
    "\n",
    "gp.fit(df[['x', 'y']].values, df.por.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make a prediction, we need to construct the X matrix: (x, y) coordinates in 2 columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_grid = np.stack([grid_x.ravel(), grid_y.ravel()]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can make a prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_grid = gp.predict(X_grid).reshape(grid_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And plot the predicted grid with the input data using the same colourmap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute min and max of all the data:\n",
    "mi = np.min(np.hstack([y_grid.ravel(), df.por.values]))\n",
    "ma = np.max(np.hstack([y_grid.ravel(), df.por.values]))\n",
    "\n",
    "# Plot it all.\n",
    "plt.figure(figsize=(15,15))\n",
    "im = plt.imshow(y_grid.T, origin='lower', extent=extent, vmin=mi, vmax=ma)\n",
    "pts = plt.scatter(df.x, df.y, c=df.por, s=80, edgecolor='#ffffff66', vmin=mi, vmax=ma)\n",
    "plt.colorbar(im, shrink=0.67)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we can compute the error by making a prediction on the original (x, y) values and comparing to the actual measured porosities at those locations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "por_hat = gp.predict(df[['x', 'y']].values)\n",
    "\n",
    "sns.distplot(por_hat - df.por)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using `verde.Spline`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the options in scipy's `Rbf` interpolator is the \"thin-plate\" kernel. This is what the `verde.Spline` interpolator is based on but with a few modifications, like damping regularization to smooth the solution. It's similar to the `RBF` and `GaussianProcessRegressor` approach but Verde provides a more conenient API for gridding tasks.\n",
    "\n",
    "For example, we now have a nicer way to define `extent`, using `vd.pad_region()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import verde as vd\n",
    "\n",
    "extent = x_min, x_max, y_min, y_max = vd.pad_region(vd.get_region((df.x, df.y)), pad=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spline = vd.Spline(mindist=2000, damping=1e-4)\n",
    "spline.fit((df.x, df.y), df.por)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make a grid, use the `.grid` method of the spline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = spline.grid(region=extent, spacing=500)\n",
    "grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This returns an [`xarray.Dataset`](http://xarray.pydata.org/en/stable/generated/xarray.Dataset.html#xarray.Dataset) which can be easily plotted, saved to disk as netCDF, or used for computations. The coordinates for the grid are automatically generated and populated based on the desired region and spacing. The spacing is adjusted to fit the desired region exactly. Optionally, you can set `adjust=\"region\"` to adjust the size of the region so that the spacing is exact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And plot the predicted grid with the input data using the same colourmap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute min and max of all the data:\n",
    "mi = np.min(np.hstack([grid.scalars.values.ravel(), df.por.values]))\n",
    "ma = np.max(np.hstack([grid.scalars.values.ravel(), df.por.values]))\n",
    "\n",
    "# Plot it all.\n",
    "plt.figure(figsize=(15,15))\n",
    "im = plt.imshow(grid.scalars, origin='lower', extent=extent, vmin=mi, vmax=ma)\n",
    "pts = plt.scatter(df.x, df.y, c=df.por, s=80, edgecolor='#ffffff66', vmin=mi, vmax=ma)\n",
    "plt.colorbar(im, shrink=0.67)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we can compute the error by making a prediction on the original (x, y) values and comparing to the actual measured porosities at those locations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "por_hat = spline.predict((df.x, df.y))\n",
    "\n",
    "sns.distplot(por_hat - df.por)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Just the essential bits\n",
    "\n",
    "This is the minimal code required to load the data and make a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import Rbf\n",
    "\n",
    "# Load the data.\n",
    "df = pd.read_csv('https://www.dropbox.com/s/6dyfc4fl5slhgry/ZoneA.dat?raw=1',\n",
    "                 sep=' ',\n",
    "                 header=9,\n",
    "                 usecols=[0, 1, 2, 3],\n",
    "                 names=['x', 'y', 'thick', 'por']\n",
    "                )\n",
    "\n",
    "# Build a regular grid with 500-metre cells.\n",
    "extent = x_min, x_max, y_min, y_max = [df.x.min()-1000, df.x.max()+1000,\n",
    "                                       df.y.min()-1000, df.y.max()+1000]\n",
    "grid_x, grid_y = np.mgrid[x_min:x_max:500, y_min:y_max:500]\n",
    "\n",
    "# Make the interpolator and do the interpolation.\n",
    "rbfi = Rbf(df.x, df.y, df.por)\n",
    "di = rbfi(grid_x, grid_y)\n",
    "\n",
    "# Make the plot.\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.imshow(di.T, origin=\"lower\", extent=extent)\n",
    "cb = plt.scatter(df.x, df.y, s=60, c=df.por, edgecolor='#ffffff66')\n",
    "plt.colorbar(cb, shrink=0.67)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "&copy; 2019 Agile Scientific, licensed CC-BY"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mastery",
   "language": "python",
   "name": "mastery"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
