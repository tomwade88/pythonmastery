{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Reading data from files\n", "\n", "Sometimes we need to read data from files. In general, these will be text files or binary files. Text files are easy to read, binary files are not.\n", "\n", "Let's start with writing and then reading a bit of text."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["Now we can read this data:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["## File modes\n", "\n", "You have to decide what you want to do with the file.\n", "\n", "- **`r`** &mdash; read only (default)\n", "- **`r+`** &mdash; read and write (pointer at 0 &mdash; careful to manage the pointer!)\n", "- **`w`** &mdash; write new file (clobbers existing files)\n", "- **`a`** &mdash; append existing\n", "\n", "You can also add another letter to indicate whether you're handling text or bytes:\n", "\n", "- **`t`** &mdash; text (default)\n", "- **`b`** &mdash; bytes\n", "\n", "For example, to open an existing text file for appending data to the end:\n", "\n", "    with open(fname, 'at') as f:\n", "        f.write('New data')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Read some data"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Let's read some tops data:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["Note that we can also use this pattern:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["with open('../data/L-30_tops.txt', 'r') as f:\n", "    for line in f:\n", "        print(line, end='')"]}, {"cell_type": "markdown", "metadata": {"tags": ["exercise"]}, "source": ["<div class=\"alert alert-success\">\n", "<h3>Exercise</h3>\n", "\n", "Write a `for` loop to read the lines of the file one by one, adding key: value pairs to a dictionary as you go.\n", "\n", "<a title=\"You will need to skip the loop over lines that look like comments. Use str.split() to break the line at a comma, and `float()` to convert strings to numbers.\">**Hover for hints**</a>", "\n</div>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# YOUR CODE HERE\n", "\n", "\n"]}, {"cell_type": "markdown", "metadata": {"tags": ["exercise"]}, "source": ["<div class=\"alert alert-success\">\n", "<h3>Exercise</h3>\n", "\n", "Your challenge is to turn this into a function, complete with docstring and any options you want to try to implement. For example:\n", "\n", "- Try putting everything, including the file reading into the function. (Better yet, write functions for each main 'chunk' of the workflow.)\n", "- When the function works on `../data/L-30_tops.txt`, make it work on `../data/B-41_tops.txt`. (Remember you have some 'depth, unit' code from Day 1.)\n", "- You could let the user choose different 'comment' characters.\n", "- Let the user select different delimiters, other than a comma.\n", "- Other things, like skipping lines or transforming the case of the names, could also be optional.\n", "- Maybe print some 'progress logging' as you go, so the user knows what's going on.\n", "- Don't forget the docstring!\n", "\n", "When you're done, add the function to `utils.py`.", "\n</div>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def clean_depth(string):\n", "    \"\"\"Clean the units from a number.\"\"\"\n", "    if 'm' in string.lower():\n", "        units = 'M'\n", "    elif 'f' in string.lower():\n", "        units = 'FT'\n", "    else:\n", "        units = None\n", "\n", "    stripped = string.lower().strip(' .mft\\n\\t')\n", "    value = stripped.replace(',', '')\n", "\n", "    return float(value), units"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["get_tops_from_file('../data/B-41_tops.txt', comment='%')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Reading multiple files\n", "\n", "Sometimes we want to crawl directories. Usually, you can accomplish this with `glob`:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import glob\n", "\n", "glob.glob('../data/*.[Ll][Aa][Ss]')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["pd.read_csv??"]}, {"cell_type": "markdown", "metadata": {}, "source": ["This is just a list of path strings, so you can loop over it to read multiple files. It supports all the usual syntax for UNIX-style globbing, including recursion over directories with `**`. "]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Intro to Python students: stop here for now\n", "\n", "----"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Reading without loops: `re`"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# With regex.\n", "import re\n", "\n", "with open('../data/L-30_tops.txt') as f:\n", "    data = re.findall('(.+?),([.0-9]+)', f.read())\n", "    tops = {name: float(depth) for name, depth in data}"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["tops"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Reading without loops: `map` and `filter`"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def process_line(line):\n", "    \"\"\"Process one valid line.\n", "    \"\"\"\n", "    k, v = line.split(',')\n", "    return k, float(v)\n", "\n", "def is_valid(line):\n", "    \"\"\"Decide is a line is processable.\n", "    \"\"\"\n", "    return '#' not in line\n", "\n", "with open('../data/L-30_tops.txt') as f:\n", "    tops = dict(map(process_line, filter(is_valid, f)))\n", "    \n", "tops"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Read using NumPy\n", "\n", "We can use `np.loadtxt()` for numeric files."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "np.loadtxt('../data/L-30_tops.txt', skiprows=1, usecols=[1], delimiter=',')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Or there's [`np.genfromtxt()`](https://docs.scipy.org/doc/numpy-1.13.0/user/basics.io.genfromtxt.html), which copes better with missing values &mdash; try running it on `'../data/B-41_tops.txt'`."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["np.genfromtxt('../data/L-30_tops.txt', skip_header=1, delimiter=',')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Both functions have a useful keyword argument, `unpack`, which you should set to `True` to get the columns back as separate vectors.\n", "\n", "Note that both functions can read GZIP files too."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## `csv` built-in module"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import csv\n", "\n", "with open('../data/L-30_tops.csv') as f:\n", "    reader = csv.reader(f)\n", "    for row in reader:\n", "        print(row)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import csv\n", "\n", "with open('../data/L-30_tops.csv') as f:\n", "    reader = csv.DictReader(f)\n", "    for row in reader:\n", "        print(row['Formation name'], row['Depth [m]'])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Read file using pandas"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import pandas as pd\n", "\n", "df = pd.read_csv('../data/L-30_tops.csv')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import pandas as pd\n", "\n", "df = pd.read_csv('../data/L-30_tops.txt', skiprows=1, names=['Formation', 'Depth'])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df['Formation'] = df['Formation'].str.title()\n", "df.head()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df.to_csv('../data/L-30_tops_improved.csv')"]}, {"cell_type": "markdown", "metadata": {"tags": ["exe"]}, "source": ["<div class=\"alert alert-success\">\n", "<h3>Exercises</h3>\n", "\n", "- Read the data from B-41_tops.txt\n", "- Write a function that will load data from either of these files\n", "- Load the data to pandas\n", "- Write a new CSV files with the cleaned data", "\n</div>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}], "metadata": {"kernelspec": {"display_name": "geocomp", "language": "python", "name": "geocomp"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.1"}}, "nbformat": 4, "nbformat_minor": 1}